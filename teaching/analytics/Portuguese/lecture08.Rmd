<style>
  .reveal pre {
    font-size: 13pt;
  }
  .reveal section p {
    font-size: 32pt;
  }
  .reveal div {
    font-size: 30pt;
  }
  .reveal h3 {
    color: #484848;
    font-weight: 150%;
  }
</style>

---
title: Lecture 8 - ANOVA and Regression
author: 94-842
date: November 13, 2014
output: html_document
---

###Agenda

- ANOVA

- Linear regression (continuous outcomes)

- Logistic regression (binary outcomes)


Let's begin by loading the packages we'll need to get started
```{r}
library(MASS)
library(plyr) 
library(ggplot2)
```

### ANOVA

You can think of ANOVA (analysis of variance) as a more general version of the t-test, or a special case of linear regression in which all covariates are factors.  

Let's go back to our favourite birthwt data set from the MASS library.

```{r}
# Rename the columns to have more descriptive names
colnames(birthwt) <- c("birthwt.below.2500", "mother.age", "mother.weight", 
    "race", "mother.smokes", "previous.prem.labor", "hypertension", "uterine.irr", 
    "physician.visits", "birthwt.grams")

# Transform variables to factors with descriptive levels
birthwt <- transform(birthwt, 
            race = as.factor(mapvalues(race, c(1, 2, 3), 
                              c("white","black", "other"))),
            mother.smokes = as.factor(mapvalues(mother.smokes, 
                              c(0,1), c("no", "yes"))),
            hypertension = as.factor(mapvalues(hypertension, 
                              c(0,1), c("no", "yes"))),
            uterine.irr = as.factor(mapvalues(uterine.irr, 
                              c(0,1), c("no", "yes")))
            )
```

##### One-way ANOVA example

**Question: Is there a significant association between race and birthweight?**

Here's a table showing the mean and standard error of birthweight by race.

```{r}
aggregate(birthwt.grams ~ race, data = birthwt, FUN = mean)
```

It looks like there's some association, but we don't yet know if it's statistically significant.  Note that if we had just two racial categories in our data, we could run a t-test.  Since we have more than 2, we need to run a 1-way analysis of variance (ANOVA). 

**Terminology**: a $k$-way ANOVA is used to assess whether the mean of an outcome variable is constant across all combinations of $k$ factors.  The most common examples are 1-way ANOVA (looking at a single factor), and 2-way ANOVA (looking at two factors).

We'll use the `aov()` function.  For convenience, `aov()` allows you to specify a formula.
```{r}
summary(aov(birthwt.grams ~ race, data = birthwt))
```

The p-value is significant at the 0.05 level, so the data suggests that there is an association between birthweight and race.  In other words, average birthweight varies across the three racial groups considered in the data.


### Linear regression

Linear regression is just a more general form of ANOVA.  It allows you to model effects of continuous variables. 

**linear regression** is used to model linear relationship between an outcome variable, $y$, and a set of *covariates* or *predictor variables* $x_1, x_2, \ldots, x_p$. 

For our first example we'll look at a small data set in which we're interested in predicting the crime rate per million population based on socio-economic and demographic information at the state level.  

Let's first import the data set and see what we're working with.

```{r}
# Import data set
crime <- read.table("http://www.andrew.cmu.edu/user/achoulde/94842/data/crime_simple.txt", sep = "\t", header = TRUE)
```

**The variable names that this data set comes with are very confusing, and even misleading.**

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean # of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income

**We really need to give these variables better names**


```{r}
# Assign more meaningful variable names
colnames(crime) <- c("crime.per.million", "young.males", "is.south", "average.ed",
                     "exp.per.cap.1960", "exp.per.cap.1959", "labour.part",
                     "male.per.fem", "population", "nonwhite",
                     "unemp.youth", "unemp.adult", "median.assets", "num.low.salary")

# Convert is.south to a factor
# Divide average.ed by 10 so that the variable is actually average education
# Convert median assets to 1000's of dollars instead of 10's
crime <- transform(crime, is.south = as.factor(is.south),
                          average.ed = average.ed / 10,
                          median.assets = median.assets / 100)

# print summary of the data
summary(crime)
```

#### First step: some plotting and summary statistics

You can start by feeding everything into a regression, but it's often a better idea to construct some simple plots (e.g., scatterplots and boxplots) and summary statistics to get some sense of how the data behaves.

```{r, fig.align='center', fig.height=4, fig.width=5}
# Scatter plot of outcome (crime.per.million) against average.ed
qplot(average.ed, crime.per.million, data = crime)
# correlation between education and crime
with(crime, cor(average.ed, crime.per.million))
```

This seems to suggest that higher levels of average education are increased with higher crime rates.  *Can you come up with an explanation for this phenomenon?*

```{r, fig.align='center', fig.height=4, fig.width=5}
# Scatter plot of outcome (crime.per.million) against median.assets
qplot(median.assets, crime.per.million, data = crime)
# correlation between education and crime
with(crime, cor(median.assets, crime.per.million))
```

There also appears to be a positive association between median assets and crime rates.

```{r, fig.align='center', fig.height=5, fig.width=5}
# Boxplots showing crime rate broken down by southern vs non-southern state
qplot(is.south, crime.per.million, geom = "boxplot", data = crime)
```

#### Constructing a regression model

To construct a linear regression model in R, we use the `lm()` function.  You can specify the regression model in various ways.  The simplest is often to use the formula specification.

The first model we fit is a regression of the outcome (`crimes.per.million`) against all the other variables in the data set.  You can either white out all the variable names. or use the shorthand `y ~ .` to specify that you want to include all the variables in your regression.

```{r}
crime.lm <- lm(crime.per.million ~ ., data = crime)
# Summary of the linear regression model
crime.lm
summary(crime.lm)
```

R's default is to output values in scientific notation.  This can make it hard to interpret the numbers.  Here's some code that can be used to force full printout of numbers.

```{r}
options(scipen=4)  # Set scipen = 0 to get back to default
```

```{r}
summary(crime.lm)
```

Looking at the p-values, it looks like `num.low.salary` (number of families per 1000 earning below 1/2 the median income), `unemp.adult` (Unemployment rate of urban males per 1000 of age 35-39), `average.ed` (Mean # of years of schooling 25 or older), and `young.males` (number of males of age 14-24 per 1000 population) are all statistically significant predictors of crime rate.  

The coefficients for these predictors are all positive, so crime rates are positively associated with wealth distribution, adult unemployment rates, average education levels, and high rates of young males in the population.  

##### Exploring the lm object

What kind of output do we get when we run a linear model (`lm`) in R?

```{r}
# List all attributes of the linear model
attributes(crime.lm)

# coefficients
crime.lm$coef
```

None of the attributes seem to give you p-values.  Here's what you can do to get a table that allows you to extract p-values. 

```{r}
# Pull coefficients element from summary(lm) object
summary(crime.lm)$coef
```

If you want a particular p-value, you can get it by doing the following

```{r}
# Pull the coefficients table from summary(lm)
crime.lm.coef <- summary(crime.lm)$coef
# See what this gives
class(crime.lm.coef)
attributes(crime.lm.coef)
crime.lm.coef["average.ed","Pr(>|t|)"]
```

The coefficients table is a matrix with named rows and columns.  You can therefore access particular cells either by numeric index, or by name (as in the example above).

##### Plotting the lm object

```{r, fig.align='center', fig.height=4.5, fig.width=6, cache=TRUE}
plot(crime.lm)
```

These four plots are important diagnostic tools in assessing whether the linear model is appropriate.  The first two plots are the most important, but the last two can also help with identifying outliers and non-linearities.  

**Residuals vs. Fitted** When a linear model is appropriate, we expect 

1. the residuals will have constant variance when plotted against fitted values; and 

2. the residuals and fitted values will be uncorrelated.  
  
If there are clear trends in the residual plot, or the plot looks like a funnel, these are clear indicators that the given linear model is inappropriate.

**Normal QQ plot** You can use a linear model for prediction even if the underlying normality assumptions don't hold.  However, in order for the p-values to be believable, the residuals from the regression must look approximately normally distributed.  

**Scale-location plot** This is another version of the residuals vs fitted plot.  There should be no discernible trends in this plot.

**Residuals vs Leverage**.  Leverage is a measure of how much an observation influenced the model fit.  It's a one-number summary of how different the model fit would be if the given observation was excluded, compared to the model fit where the observation is included.  Points with *high residual* (poorly described by the model) and *high leverage* (high influence on model fit) are outliers.  They're skewing the model fit away from the rest of the data, and don't really seem to fit with the rest of the data.

##### Collinearity

In your regression class you probably learned that **collinearity** can throw off the coefficient estimates.  To diagnose collinearity, we can do a plot matrix.  In base graphics, this can be accomplished via the `pairs` function.

As a demo, let's look at some of the economic indicators in our data set.

```{r}
economic.var.names <- c("exp.per.cap.1959", "exp.per.cap.1960", "unemp.adult", "unemp.youth", "labour.part", "median.assets")
pairs(crime[,economic.var.names])
round(cor(crime[,economic.var.names]), 3)
```

Looking at the plot and correlation matrix, we see that many of the variables are very strongly correlated.  In particular, police expenditures are pretty much identical in 1959 and 1960.  This is an extreme case of collinearity.  Also, unsurprisingly, youth unemployment and adult unemployment are also highly correlated.  

Let's just include the 1960 police expenditure variable, and also drop the young unemployment variable.  We'll do this using the `update()` function.  Here's what happens.

```{r}
crime.lm.2 <- update(crime.lm, . ~ . - exp.per.cap.1959 - unemp.youth)
summary(crime.lm.2)
crime.lm.summary.2 <- summary(crime.lm.2)
```

##### Thinking more critically about the linear model

We see that `exp.per.cap.1960` is now highly significant.  
```{r}
crime.lm.summary.2$coef["exp.per.cap.1960",]
```

This is interesting.  It's essentially saying that, all else being equal, every dollar per capita increase in police expenditure is on average associated with an increase in crime of 1.13 per million population.  

```{r}
crime.lm.summary.2$coef["average.ed",]
```

Also, for every unit increase in average education, we find that the number of reported crimes increases by about 15.3 per million.  

One of my main reasons for selecting this data set is that it illustrates some of the more common pitfalls in interpreting regression models.  

**Just because a coefficient is significant, doesn't mean your covariate causes your response**

- This is the old adage that correlation does not imply causation.  In this example, we have strong evidence that higher police expenditures are positively associated with crime rates.  This doesn't mean that decreasing police expenditure will lower crime rate.  The relationship is not causal -- at least not in that direction.  A more reasonable explanation is that higher crime rates promt policy makers to increase police expenditure.

**There's a difference between practical significance and statistical significance**

- Both `average.ed` and `exp.per.cap.1960` are statistically significant.  `exp.per.cap.1960` has a much more significant p-value, but also a much smaller coefficient.  When looking at your regression model, you shouldn't just look at the p-value column.  The really interesting covariates are the ones that are significant, but also have the largest effect.  


### Logistic regression

You probably saw linear regression at some point in your statistics education.  You may not have seen **logistic regression**, which is used when the outcome is binary.  

For this example we'll use an Income data set where the goal is to predict whether an individual's income is under 50k or over 50k.

```{r, cache=TRUE}
# Import data file
income <- read.csv("http://www.andrew.cmu.edu/user/achoulde/94842/data/income_data.txt", header=FALSE)

# Give variables names
colnames(income) <- c("age", "workclass", "fnlwgt", "education", "education.years", "marital.status", "occupation", "relationship", "race", "sex", "capital.gain", "capital.loss", "hours.per.week", "native.country", "income.bracket")

# Create a 0-1 indicator of whether income is higher than 50K
income <- transform(income, high.income = as.numeric(income.bracket == ">50K"))
# Data dimensions
dim(income)
# Data summary
summary(income)
```

To make the problem more interesting, we'll focus only on the US data.

```{r}
income.us <- subset(income, subset = native.country == "United-States")
dim(income.us)
```

We have some categorical predictors in the data, and the default in R is to output coefficients that compare to the level that's coded 1. We'll want to use `relevel` to set a reasonable base level as the reference.  We'll relevel all the factors so that the comparison is being made to the most frequently occuring category.

```{r}
# If x is a factor, this function sets the first level to be the most frequent category
# If x is not a factor, this function returns x unchanged
relevelToMostFrequent <- function(x) {
  if (is.factor(x)) {
    # Determine most frequent category
    most.freq.level <- levels(x)[which.max(table(x))]
    # Relevel to set most frequent category as the baseline
    relevel(x, ref = most.freq.level)
  } else {
    x
  }
}
# Re-level the data
income.us.releveled <- lapply(income.us, FUN = relevelToMostFrequent)
```

#### Running a logistic regression.

To run a logistic regression, you need to use the `glm()` function (Generalized Linear Model), and specify `family = binomial()`.

```{r}
income.glm <- glm(high.income ~ age + workclass + education.years + marital.status + race + sex + hours.per.week, family = binomial(), data = income.us.releveled)
```

Let's see what we get

```{r}
summary(income.glm)
coef.table <- summary(income.glm)$coef
```

That's not very fun to try to read... let's output to a nicer table format so that we can look at that instead.

```{r, results = 'asis'}
library(knitr)  # Will use to display output more nicely
kable(coef.table, digits=3, format = "markdown")
```

For interpreting the coefficients, we should remind ourselves what the baselines are.

```{r}
sapply(income.us.releveled, FUN = function(x) { levels(x)[which.max(table(x))] })
```


#### Interpreting coefficients of logistic regression

Last class we talked a bit about odds ratios.  The coefficients in a logistic regression model are all log-odds-ratios.  

For example, the coefficient of education is `r income.glm$coef["education.years"]`.  This means that, in the data, for every 1 year increase in education level, the log-odds of having an income higher than 50k increase by `r income.glm$coef["education.years"]`.  

This may be hard to interpret, so we can exponentiate the coefficient to get something nicer.  Exponentiating gives: `r exp(income.glm$coef["education.years"])`. 

**Interpretation:** For every year of additional education, the odds of having an income over 50k increase by a factor of `r exp(income.glm$coef["education.years"])`.


For categorical variables, the interpretation is relative to the given baseline.  Let's look at the effect of marital status on the chances of having a high income.

```{r}
divorced.coef <- income.glm$coef["marital.statusDivorced"]
divorced.coef
```

The coefficient is `r divorced.coef`, and the baseline category is "married and living with spouse".  This means that the log-odds of having an income >50K are `r abs(divorced.coef)` lower for divorced people than for married people.  (Exponentiating). Equivalently, the odds of earning >50K when divorced are `r exp(divorced.coef)` times the odds of earning >50K per year.

#### Assessing significance of factors in regression

When dealing with multi-level factors, significance is assessed by considering dropping the entire factor from the regression.  You can therefore wind up in a situation where a factor is significant even when none of the individual estimated coefficients are on their own significant.  

Comparing models is the same for linear models as logistic models.  You so by using the `anova()` function and specifying two models, one of which is nested in the other.

Here's how we can test whether `race` is a significant predictor of high income.

```{r}
# Check if including the race variable significantly increases model fit
anova(update(income.glm, . ~ . - race), income.glm, test = "Chisq")
```

This returns a p-value based on a Chi-square variable with degrees of freedom = (# levels in factor - 1)

The test is highly significant, so race is indeed a signfiicant predictor of high income.