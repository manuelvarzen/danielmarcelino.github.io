---
title: Lecture 12 - Interfacing R with other tools Part I
author: 94-842
date: December 2, 2014
output: html_document
---

### Agenda

- Discussion of final project

- Common import/export tasks
  - Importing data from other common statistics software 
  - Exporting data from R
  - Exporting graphics from R
  
- R with SQL
  - SQL queries on data frames using the `sqldf` package
  - `RSQLite`, `RMySQL` 

### Final project

#### Dealing with missing values

There is a fair bit of missingness in the data set.  There are several approaches to dealing with missing data:

1. **Exclude**

  - You can omit observations with missing values (e.g., remove any rows that contain missing data)
  
2. **Impute**

  - R has various packages (Amelia, mice, mi, `impute()` function from Hmisc, etc.) that can help with imputing missing values.  
  
  - Imputation methods often rely on fairly strong assumptions concerning the process governing the appearance of missing values (assumptions such as MAR, missing at random; or MCAR, missing completely at random).  
  
  - This is a lot of hassle to go through unless you want practice imputing values
  
3. Think carefully about whether certain kinds of missingness are **informative**

  - For factor variables, you can treat missing values as just another factor level. Sometimes missingness can be informative (predictive), leading to a significant coefficient for the missing level. 
  
  - Some of the variables in the NLSY data have very interesting missingness patterns that you should think about carefully (e.g., the number of children variable has about half of the values missing... what's special about that half?)
  

I would recommend thinking carefully about missing values and then excluding them if nothing interesting is happening.  Trying to impute can consume a lot of time without producing better results than what you'd have if you just took option (1) and excluded all observations with missing values.

#### Dealing with truncated outcomes

The income variable that you have available is truncated from above.  In other words, what you observe isn't income, but $\min(INCOME, T)$, where $T$ is a high threshold.  This means that for reported incomes that are higher than $T$, all you're told is that the income was at least as high as $T$.  

If you run a regression with a censored (truncated) outcome but don't treat the truncated values in a special way, your regression estimate will be *inconsistent*.  This means that even if you had infinite data, your coefficient estimates won't converge to the "true" coefficients.  

There are several ways of dealing with censored outcomes.

1. Tobit regression (censored regression).  

  - We didn't talk about this method in class, but it's not difficult to understand if you already understand linear regression.  [A tutorial can be found here](http://www.ats.ucla.edu/stat/r/dae/tobit.htm). 
  
2. Try fitting two models, one which contains the truncated observations, and the other that doesn't.  If your estimates change a lot, then you probably don't want to use the truncated observations.  Do the same for hypothesis tests that have borderline p-values.  


My recommendation would be to take approach (2).  We didn't talk about tobit models in class, and I don't expect you to go out and learn a new regression technique just for the project.  Approach (2), on the other hand, will provide good practice in carefully taking subsets of data.

### Common import/export tasks

#### xlsx package

Many of you deal with data that's been manually entered into an Excel spreadsheet and saved in xlsx format.  While you can try to export the spreadsheet as a csv file, this doesn't always work out well (often fails if you have formulas).  

It's generally better and easier to use the `read.xlsx()` function from the `xlsx` library.  Here's the documentation header for the function.

    Usage
        read.xlsx(file, sheetIndex, sheetName=NULL, rowIndex=NULL,
        startRow=NULL, endRow=NULL, colIndex=NULL,
        as.data.frame=TRUE, header=TRUE, colClasses=NA,
        keepFormulas=FALSE, encoding="unknown", ...)

A basic import call for a 1-sheet xlsx file with headers would simply look like

```{r, eval = FALSE}
library(xlsx)
my.df <- read.xlsx("spreadsheet.xlsx", header = TRUE)
```

where  `spreadsheet.xlsx` has of course been replaced with the actual name of your xlsx file.

You can also use the `write.xlsx(x = my.df, file = "spreadsheet.xlsx", sheetName="Sheet1", ...)` to output your data frame to an xlsx file.  

This can be useful if you've put a lot of effort into cleaning your data in R, and you want to save your work in an Excel spreadsheet.

#### foreign package

The `foreign` package allows you to import data from various other kinds of statistical software.  Some of the options are tabulated below.

Function name       |  Description
-------------|-----------------
`read.dta`     |   Read Stata binary files
`read.mtp`     | Read a Minitab Portable Worksheet
`read.octave`  |  Read Octave Text Data Files
`read.spss`    |   Read an SPSS data file
`read.ssd`     |   Obtain a Data Frame from a SAS Permanent Dataset, via read.xport
`read.xport`   |  Read a SAS XPORT Format Library
`write.dta`    |  Write Files in Stata Binary Format
`write.foreign` | Write text files and code to read them

These functions are particularly useful for going back and forth between R and another language.  E.g., you may want to run an analysis in Stata and then output the results into R to produce tables and graphics.

#### Saving variables from your workspace

- Fitting models to large data sets can take a lot of computation time

- Instead of re-running all of your code every time you re-open R, you can save variables from your workspace (e.g., save that list of linear models that took `plyr` 30 minutes to produce)

- To save your **entire workspace**, use 

```{r, eval = FALSE}
save.image(file = "myworkspace.RData")
```

- To save a particular **set of variables**, use 

```{r, eval = FALSE}
save(..., file = "myvariables.RData")
```

- (Replace ... with variable names)

- E.g., if you have a data frame `df` and a linear model list `lm.list` that you want to store for later use, you can save by writing

```{r, eval = FALSE}
save(df, lm.list, file = "myvariables.RData")
```

- To **load** saved variables, just use the `load()` function

```{r, eval = FALSE}
load(file = "myvariables.RData")
```

####  Exporting tables 


- Suppose you have a data frame or matrix that contains variable summaries (e.g., you have some output from your `generateDataSummary()` function from Homework 4)

- To save the table, you can use `write.table()` or `write.csv()`

```{r, eval = FALSE}
data.summary <- generateDataSummary(df, var.names, group.name)

# write.table approach, output as tab-delimited 
write.table(data.summary, file = "mytable.txt", sep = "\t")

# write.csv approach, output as csv file
write.table(data.summary, file = "mytable.csv")
```

####  Exporting graphics

- By default, plots are output to your graphics window (Plots tab in RStudio)

- You can save plots manually from the Plots tab by clicking  `Export > Save as PDF`

- To save them automatically you can use `bmp()`, `jpeg()`, `png()`, `tiff()` or `pdf()`.  

- `pdf()` gives nice vectorized graphics, so it's often the best one to use

#####  Exporting graphics: pdf()

- The basic structure is as follows

```{r, eval = FALSE}
pdf(file = "myplot.pdf", width = 6, height = 6) # Start a pdf graphics device
    # Code 
    # to
    # generate
    # plot
dev.off()  # Close the pdf graphics device
```

Here's an example that saves plots in a loop

```{r}
library(ggplot2)
```
```{r, cache = TRUE}
# Generate and save a scatterplot of price vs carat for every level of cut
for(cut.level in levels(diamonds[["cut"]])) {
  file.name <- paste("./figures/", cut.level, ".pdf", sep = "") # Define file name based on cut
  pdf(file = file.name, width = 6, height = 5)  # Open pdf graphics device
  print(qplot(carat, price, colour = color, data = subset(diamonds, subset = cut == cut.level)))
  dev.off() # Close pdf graphics device
}
```

### R with SQL

- Most businesses use some kind of database for storing their data (commonly SQL)

- We'll discuss two topics:
  - Using `sqldf` package to interact with R data frames with SQL queries
  - Using `RSQLite` and `RMySQL` to interact with SQL databases (on **Thursday**)
  
- Why bother with databases?
  - R data frames exist in **memory**, not on your hard disk
  - If you're only using a small fraction of your data, this is wasteful
  - Databases store data on your disk, and let you load into memory only the parts you need

#### sqldf package


- If you already know some SQL, you might find R's data manipulation functions somewhat unnatural 

- `sqldf` allows you to use SQL queries on your data frames

- The data frame is still stored in memory!  (You can just query it in a different way)

#### Examples

```{r}
# Note, if you don't have X11 on your Mac, loading sqldf may crash your machine
# Updating to the most recent version of R and installing the most recent version
# of the sqldf package generally fixes this
library(sqldf)
```

##### Basic subsetting of data

Let's go back to our gapminder data from the plyr class

```{r}
gapminder <- read.delim("http://www.andrew.cmu.edu/user/achoulde/94842/data/gapminder_five_year.txt") # Load the data
```

#### Pulling subsets

```{r}
# head(gapminder, 5) in SQL 
sqldf('select * from gapminder desc limit 5') 
# Select year, lifeExp and gdpPercap for Canada
sqldf('select year, lifeExp, gdpPercap from gapminder where country == "Canada"')
# and/or use
sqldf('select country, year, lifeExp, gdpPercap from gapminder where country == "Canada" or country == "United States"')

# Subset on a certain set of years, and order by continent (displays first 20 rows)
sqldf('select country, year, pop from gapminder where year between 1970 and 1980 order by continent desc limit 20')

# Here's the same statement, but in an easier to parse layout
sqldf('select country, year, pop from gapminder 
      where year between 1970 and 1980 
      order by continent 
      desc limit 20')
```

#### Simple summaries

We can essentially replace the aggregate function

```{r}
# Using aggregate to get average GDP by continent
aggregate(gdpPercap ~ continent, FUN = mean, data = gapminder)

# Using SQL syntax to get average GDP by continent
sqldf('select continent, avg("gdpPercap") from gapminder
      group by continent')

# Using SQL syntax to get average GDP by continent for years between 1990 and 2010
sqldf('select continent, avg("gdpPercap") from gapminder 
      where year between 1990 and 2010
      and pop > 5000000
      group by continent')

# To do the same thing in R, we'd need to write something like
new.df <- subset(gapminder, subset = pop > 5000000 & (year >= 1990 & year <= 2010)) # Get subset data frame
aggregate(gdpPercap ~ continent, FUN = mean, data = new.df)
```

