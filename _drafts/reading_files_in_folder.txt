
# 
Every campaign cycle I usually do similar things, go to a repository, download a bounce of data, merge and store them to an existing RData file for posterior analysis. I've already wrote about this topic some time ago, but this time I think my script became simpler. 

## Set the Directory
Let's assume you're not in the same directory of your files, so you'll need to set R to where the population of files resides.

    setwd("~/Downloads/consulta_cand_2014")

## Getting a List of files
Next, it’s just a matter of getting to know your files. For this, the list.files() function is very handy, and you can see the file names right-way in your screen. Here I'm looking form those "txt" files, so I want my list of files exclude everything else, like pdf, jpg etc.

    files <- list.files(pattern= '\\.txt$')

In case you have the same files in more than one format, say txt and csv, you may want trying something like the following:

    CSVs <-list.files(pattern='csv')
    TXTs <- list.files(pattern='txt')
    mylist <- CSVs[!CSVs %in% TXTs]

## Stacking files into a dataframe 
The last step is to iterate "rbind" through the list of files in the working directory putting all them together. 
Notice that in the script below I've included some extra conditions to avoid problems reading the files I have. Also, this assumes all the files have the same number of columns, otherwise "rbind" won't work. In this case you may need to replace "rbind" by "smartbind" from gtools package.

    cand_br <- do.call("rbind",lapply(files,
    FUN=function(files){read.table(files,
    header=FALSE, sep=";",stringsAsFactors=FALSE, fileEncoding="cp1252", fill=TRUE,blank.lines.skip=TRUE)}))

Sometimes you may find empty files that may prevent this economical script to run successfully. Therefore, you may want to inspect the files beforehand:

    info = file.info(files)
    empty = rownames(info[info$size == 0, ])
	
	



for (i in 1:length(file)){
       
  # if the merged dataset doesn't exist, create it
  if (!exists("dataset")){
  # these data don't header
    dataset <- read.table(files[i], header=FALSE, sep=";", stringsAsFactors=FALSE, fileEncoding="cp1252",fill=TRUE,blank.lines.skip=TRUE)
  }
   
  # if the merged dataset does exist, append to it
  if (exists("dataset")){
    temp_dataset <-read.table(files[i], header=FALSE, sep=";",stringsAsFactors=FALSE, fileEncoding="cp1252",fill=TRUE,blank.lines.skip=TRUE)
    dataset<-rbind(dataset, temp_dataset)
    rm(temp_dataset)
  }
 
}


The Full Code
Here’s the code in it’s entirety, put together for ease of pasting. I assume there are more efficient ways to do this, but it hasn’t taken long to merge 45 text files totalling about 400MB with some 300,000 rows and 300 columns.


dataset <- do.call("rbind",lapply(file_list,
FUN=function(files){read.table(files,
header=FALSE, sep=";",stringsAsFactors=FALSE, fileEncoding="latin1", fill=TRUE,blank.lines.skip=TRUE,quote="\"")}))


library(plyr)
file_list <- list.files()
dataset <- ldply(file_list, read.table, header=FALSE, sep=";", stringsAsFactors=FALSE, fileEncoding="cp1252", fill=TRUE,blank.lines.skip=TRUE,quote="")


ldply takes a list and a function as its first two inputs, plus arguments to be passed on to that function, and returns a dataframe.


Just to update this article from several years ago, these days it’s much faster to use fread from the data.table package :
require(data.table)
dataset = rbindlist(lapply( file_list, fread ))
It works out the header and separator for you, but you can pass them as well if you like :
dataset = rbindlist(lapply( file_list, fread, header=TRUE, sep=”\t” ))


myDir <- "some file path" 
filenames <- list.files(myDir) 
filenames <- filenames[grep("[.]csv", filenames)] 

data_names <- gsub("[.]csv", "", filenames) 

for(i in 1:length(filenames)) assign(data_names[i], read.csv(file.path(myDir, filenames[i])))
