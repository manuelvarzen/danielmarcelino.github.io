 <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 
 Title:  Notes on Kalman Filters 
Author: Daniel Marcelino
Date:   July 25, 2014 

####Tracking
The *kalman filter* is a popular technique for estimating state space and hidden structures. A Kalman filter essentially estimates a continuous state and gives a uni-modal distribution. 

The Google self-driving car uses methods like these to understand where the traffic is based on radar data.

$$f(x)=\frac{1}{\sqrt{2\pi\sigma2}}~exp-\frac{1}{2}\frac{(x-\mu)2} {\sigma2}$$

The Kalman Filtering process seeks to discore an underlying set of state variables $x_k$ given a set of measurements $y_k$. The process and measurment equations are both linear and given by 
$$ x_{n+1} = F_n x_n + v_{o,n+1}$$
$$ y_{n} = \Phi  x_n + v_{d,n.}$$

The Kalman filter wants to find, at each iteration, the most likely cause of the measurement $y_n$ given the approximation made by a flawed estimation (the linear dynamics $F_n$. Figure shows a 2-dimensional graphical depiction. What is important here is not only that we have the measurement and the prediction, but knowledge of how each is flawed. In the Kalman case, this knowledge is given by the covariance matrixes (essentially fully describing the distribution of the measurement and prediction for the Gaussian case). In Figure, this knowledge is represented by the ovals surrounding each point. The power of the Kalman filter comes from it's ability not only to perform this estimation once (a simple Bayesian task), but to use both estimates and knowledge of their distributions to find a distribution for the updated estimate, thus iteratively calculating the best solution for state at each iteration.

![KalmanDiagram.png](http://cnx.org/resources/4f4af9261ec0a51006949f1f53e075d7/KalmanDiagram.png)

#MCMC
##The Burn-in
The theory says the chain will reach the required distribution eventually
... but does not say when
Thus, we remove the first few values to be in the safe side.
##The rest
When the posterior has reached the stationary distribution, we say it has converged.
However, even after we have reached convergence, the chain might not be good:
# Solutions
Run the chain for a longer period.
e.g. 100 times as long, see if you get good mixing.
#Long Chains 
If we run an chain for a long time, then we get lost of numbers, which takes up a lot of memory.
Therefore, we want to take only some iterations:
e.g. take every 10 (thinning interval)
Taking iterations at an even interval reduces the autocorrelation between iterations.
#How many point?
To get a good coverage of the target distribution, we need a lot of points.
But the precise number will depend on what you want and what resources you have:
-1000 iterations might be ok for some problems.
Want to cover most of the distribution?
-more dimensions will need more iterations.
We can assess how well the estimation is doing by comparing the mean of the samples, and the true posterior mean.
#Checking the chain
We need to check if we have converged.
There are formal methods, mostly based on running several chains.
If we have not converged, then we may have to remove our points, as a burn-in.
Or, if we have converged, but the mixing is not good, then we have to either thin the chain, or improve the sampling.


Time Series

Markov Switching models can be fitted by expectation maximization (EM): estimate the state at each time; then the parameters in each state; iterate.

You can mine a large number of time series by computing a few metrics for each of them (average, volatility, minimum and maximum of the derivative, etc. -- use the methods of functional data analysis (FDA)) and plot the resulting multivariate dataset (1- or 2-dimensional plots, PCA (principal component analysis), etc.). This is the idea behind scagnostics, applied to time series.

In a threshold autoregressive model (TAR) model, the autogression (AR) coefficient depends on the previous value (often, one allows two values depending on the sign; sometimes three values corresponding to ``significantly positive'', ``significantly negative'', ``small''). The idea can be generalized to threshold cointegration: two time series are threshold cointegrated if they have a threshold stationary linear combination y, i.e.,

t(t+1) = alpha(y(t)) y_t + epsilon(t+1)
with

alpha(y(t)) < 1  if  y(t) is large
The Kalman filter can be robustified to resist to either outliers or regime changes (or even both, but with a delay) with the rLS and ACM algorithms.

With the seewave package, R can process sound samples (the plots are nice and the PDF presentation even contained an animation) -- of course, since R tends to store data in memory, this is only limited to sound samples.



Univariate data

To estimate the mode of unimodal data, you can compute density estimators with wider and wider kernels, look at their local maxima and remember their positions when they disappear: you get a dendogram-like plot -- this can actually ge generalized to higher dimensional data (apply some dimension reduction algorithm afterwards, if needed): check the denpro and delt packages.

The DTDA package provides functions to analyze truncated data (estimator of the cumulated distribution function (cdf), etc.).

The fitdistrplus package helps you fit distributions (as MASS::fitdistr), but allows for censored observations, provides skewness/kurtosis plots (with the dataset, bootstrap replications, and the regions attainable by the models you are considering) and goodness-of-fit tests (Anderson-Darling test to compare the data and the fitted model -- it accounts for estimated parameters).

The mhurdle package generalizes the Tobit model

y1  = beta * x + epsilon
y0  = y1 * ifelse(y1>0, 1, 0)
to multiple hurdle models

y  = beta * x + epsilon
y0 = y1 * ifelse(y[,1]>0 & y[,2]>0 & ... & y[,n]>0, 1, 0)

