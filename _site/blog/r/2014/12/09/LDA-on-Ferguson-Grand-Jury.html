<!DOCTYPE html>
<html lang="en">
	<head>
        <title>LDA on Ferguson Grand Jury I | Daniel Marcelino</title>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
        <meta name="keywords" content="methods, data analysis, R programming" />
        <meta name="author" content="Daniel Marcelino" />
        <meta name="description" content="Website of Daniel Marcelino" />
        <meta name="viewport" content="width=640px, initial-scale=1.0, maximum-scale=1.0">	
        <!-- favicon -->
        <link rel="shortcut icon" href="/images/favicon.ico">
        <!-- Google Fonts -->
		<link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,300,400,600" rel="stylesheet" type="text/css">
		<link href='http://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css" type="text/css" media="screen, projection" />
		<link rel="alternate" type="application/rss+xml" title="EnfoRced Data Analysis" href="/feed.xml">
	<link rel="alternate" type="application/rss+xml" title="EnfoRced Data Analysis" href="/feed.r.xml">
		<link rel="alternate" type="application/atom+xml" title="EnfoRced Data Analysis" href="/atom.xml" />
		
	
</head>

	<body>


		<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="header-page">

<h1><a href="/"><span style='font-weight: bold;'>Daniel Marcelino</span></a></h1></span>
<div id="nav">
	<ul>
	   <li><a href="/blog" class="current"><strong>Blog</strong></a></li>
		<li><a href="/about" class=""><strong>About</strong></a></li>
	<li><a href="/research" class=""><strong>Research</strong></a></li>
		<li><a href="/teaching" class=""><strong>Teaching</strong></a></li>
		<li><a href="/software" class=""><strong>Software</strong></a></li>		
	<li><a href="/randomwalks" class=""><strong>Random Walks</strong></a></li>		
	</ul>
</div>

</div>


<div class="clear"></div>

<div id="block">
	<div class="prose"> 
	<div class="post">
	<h1> LDA on Ferguson Grand Jury I </h1>
	<!-- <span class="date">Posted on 09 Dec 2014</span> -->
	<h5>Posted on December 09, 2014</h5>
	
	<p>The case of Michael Brown, an unarmed black teenager, who was shot and killed on August 9th, by Darren Wilson, a white police officer, in Ferguson has driven public opinion around the globe to the suburb of St. Louis.</p>

<!--more-->

<p><img src="/images/blog/2014/ferguson.jpeg" alt="LDA" /></p>

<h3 id="motivation">Motivation</h3>
<p>The case of Michael Brown, an unarmed black teenager, who was shot and killed on August 9th, by Darren Wilson, a white police officer, in Ferguson has driven public opinion around the globe to the suburb of St. Louis. After few weeks, on Nov. 24, the St. Louis County prosecutor announced that a grand jury decided not to indict Mr. Wilson. This announcement triggered another ongoing wave of protests.</p>

<p>This trial yields to significant amount of text, which soon became available over the <a href="http://twitter.com/MitchFraas">internet</a>. Thanks for work-horse on the text files, now I can simply <a href="https://s3.amazonaws.com/fraasdev/FergusonTextGuide.txt">download</a> and analyze them.</p>

<p>I spent few hours learning about LDA–Latent Dirichlet Allocation from a package called Mallet’. The Mallet machine learning package provides an interface to the Java implementation of latent Dirichlet allocation. To process a text file into mallet` a spot list of words file is required. Typically a file with unimportant words and tag marks that can instruct the algorithm.</p>

<h4 id="important-packages">Important packages</h4>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">library</span><span class="p">(</span>devtools<span class="p">)</span>
<span class="kn">library</span><span class="p">(</span>repmis<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>dplyr<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>mallet<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>cluster<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>wordcloud<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>corrgram<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>ellipse<span class="p">)</span>
<span class="kn">require</span><span class="p">(</span>RColorBrewer<span class="p">)</span></code></pre></div>

<h4 id="the-dictionary">The dictionary</h4>

<div class="highlight"><pre><code class="language-r" data-lang="r">data_url <span class="o">&lt;-</span> <span class="s">&#39;https://github.com/danielmarcelino/Tables/raw/master/en.txt&#39;</span>

stop <span class="kt">list</span> <span class="o">&lt;-</span> repmis<span class="o">::</span>source_data<span class="p">(</span>data_url<span class="p">,</span> sep <span class="o">=</span> <span class="s">&quot;,&quot;</span><span class="p">,</span> header <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span></code></pre></div>

<p>Having downloaded the documents, let’s import them from the folder. Each document is split as 1000 words chunks. To proceed, we write the stop list file to the path directory.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">docs <span class="o">&lt;-</span> mallet.read.dir<span class="p">(</span><span class="s">&quot;FergusontText/chunks&quot;</span><span class="p">)</span>

write.table<span class="p">(</span>stop <span class="kt">list</span><span class="p">,</span> file<span class="o">=</span><span class="s">&quot;stoplist.txt&quot;</span><span class="p">,</span>quote<span class="o">=</span><span class="bp">F</span><span class="p">,</span> sep<span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">,</span> row.names<span class="o">=</span><span class="bp">F</span><span class="p">)</span>

mallet.docs <span class="o">&lt;-</span> mallet.import<span class="p">(</span>docs<span class="o">$</span>id<span class="p">,</span> docs<span class="o">$</span>text<span class="p">,</span> <span class="s">&quot;en.txt&quot;</span><span class="p">,</span> token.regexp <span class="o">=</span> <span class="s">&quot;\p{L}[\p{L}\p{P}]+\p{L}&quot;</span><span class="p">)</span></code></pre></div>

<p>Let’s create a topic trainer object for data</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">n.topics <span class="o">&lt;-</span> <span class="m">50</span> topic.model <span class="o">&lt;-</span> MalletLDA<span class="p">(</span>n.topics<span class="p">)</span></code></pre></div>

<p>And then load the documents:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic.model<span class="o">$</span>loadDocuments<span class="p">(</span>mallet.instances<span class="p">)</span></code></pre></div>

<p>Now we can actually get the vocabulary and few statistics about word frequencies.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">vocab <span class="o">&lt;-</span> topic.model<span class="o">$</span>getVocabulary<span class="p">()</span>

word.freq <span class="o">&lt;-</span> mallet.word.freqs<span class="p">(</span>topic.model<span class="p">)</span>

word.freq <span class="o">%&gt;%</span> arrange<span class="p">(</span>desc<span class="p">(</span>term.freq<span class="p">))</span> <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">10</span><span class="p">)</span></code></pre></div>

<h4 id="almost-there">Almost there</h4>
<p>Nice, we have all set. Let’s use simulations to optimize hyper parameters every 25 iterations with a warm-up period of 100 iterations (by default, the hyper parameter optimization is on). After this we can actually train the model. Once again, we can specify the number of iterations, or draws after the burn-in. I’m specifying 200 draws.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic.model<span class="o">$</span>setAlphaOptimization<span class="p">(</span><span class="m">25</span><span class="p">,</span> <span class="m">100</span><span class="p">)</span>

topic.model<span class="o">$</span>train<span class="p">(</span><span class="m">200</span><span class="p">)</span></code></pre></div>

<p>Now it runs through only few iterations, but picking the ‘best’ topic for each token rather than sampling from the posterior distribution.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic.model<span class="o">$</span>maximize<span class="p">(</span><span class="m">20</span><span class="p">)</span></code></pre></div>

<p>Notice that we a structure like: words nested topics, and topics in documents. Thus, it might be a good idea to get the probability of topics in documents and the probability of words in topics.</p>

<p>There is no magic here. The following functions return raw word counts, as I want probabilities, I’ve to normalize them. I also add smoothing to that so to avoid seen some topics with exactly 0 probability.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">doc.topics <span class="o">&lt;-</span> mallet.doc.topics<span class="p">(</span>topic.model<span class="p">,</span> smoothed<span class="o">=</span><span class="bp">T</span><span class="p">,</span> normalized<span class="o">=</span><span class="bp">T</span><span class="p">)</span>

topic.words <span class="o">&lt;-</span> mallet.topic.words<span class="p">(</span>topic.model<span class="p">,</span> smoothed<span class="o">=</span><span class="bp">T</span><span class="p">,</span> normalized<span class="o">=</span><span class="bp">T</span><span class="p">)</span></code></pre></div>

<p>Now I’ve two matrixes to transpose and normalize the doc:topics</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic.docs <span class="o">&lt;-</span> <span class="kp">t</span><span class="p">(</span>doc.topics<span class="p">)</span>

topic.docs <span class="o">&lt;-</span> topic.docs <span class="o">/</span> <span class="kp">rowSums</span><span class="p">(</span>topic.docs<span class="p">)</span>

Write down the output as CSV <span class="kr">for</span> further analysis<span class="o">:</span>

write.csv<span class="p">(</span>topic.docs<span class="p">,</span> <span class="s">&quot;ferguson-topics.csv&quot;</span> <span class="p">)</span></code></pre></div>

<p>Now we can obtain a vector with short names for the topics:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topics.labels <span class="o">&lt;-</span> <span class="kp">rep</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">,</span> n.topics<span class="p">)</span>

<span class="kr">for</span><span class="p">(</span>topic <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>n.topics<span class="p">)</span> topics.labels<span class="p">[</span>topic<span class="p">]</span> <span class="o">&lt;-</span> <span class="kp">paste</span><span class="p">(</span>mallet.top.words<span class="p">(</span>topic.model<span class="p">,</span> topic.words<span class="p">[</span>topic<span class="p">,],</span> num.top.words<span class="o">=</span><span class="m">5</span><span class="p">)</span><span class="o">$</span>words<span class="p">,</span> collapse<span class="o">=</span><span class="s">&quot; &quot;</span><span class="p">)</span></code></pre></div>

<p>Check out the keywords for each topic:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topics.labels <span class="o">%&gt;%</span> <span class="kp">head</span><span class="p">(</span><span class="m">50</span><span class="p">)</span>

write.csv<span class="p">(</span>topics.labels<span class="p">,</span> <span class="s">&quot;ferguson-topics-lbs.csv&quot;</span><span class="p">)</span></code></pre></div>

<h4 id="correlation-matrix">Correlation matrix</h4>
<p>Here, Correlations with significance levels – each 1000 line chunk correlated against the others. Positive correlation – similar topics.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">cor.matrix <span class="o">&lt;-</span> cor<span class="p">(</span>topic.docs<span class="p">,</span> use<span class="o">=</span><span class="s">&quot;complete.obs&quot;</span><span class="p">,</span> method<span class="o">=</span><span class="s">&quot;pearson&quot;</span><span class="p">)</span>
write.csv<span class="p">(</span>cor.matrix<span class="p">,</span> <span class="s">&quot;corr-matrix.csv&quot;</span><span class="p">)</span></code></pre></div>

<p>From here, a variety of analyses can be conducted. As an instance, one could approach this as a network diagram, showing which pieces of the testimony share similar patterns of discourse, which ones do not.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">corrgram<span class="p">(</span>cor.matrix<span class="p">,</span> order<span class="o">=</span><span class="kc">NULL</span><span class="p">,</span> lower.panel<span class="o">=</span>panel.shade<span class="p">,</span>
upper.panel<span class="o">=</span><span class="kc">NULL</span><span class="p">,</span> text.panel<span class="o">=</span>panel.txt<span class="p">,</span>
main<span class="o">=</span><span class="s">&quot;Correlated chunks of the Ferguson&#39;s grand jury testimony&quot;</span><span class="p">)</span></code></pre></div>

<h4 id="gran-finale">Gran Finale</h4>
<p>How about turn those bits into word clouds of the topics? A word cloud can be tricky as it doesn’t tell us much of anything if obvious words are included. That’s make our stop-list file key for generating good word clouds. Of course the subject names are going to show up a lot, but a word cloud is a lot more fancy and informative if it brings what sorts of emotional or subjective language is being used.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kr">for</span><span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">){</span>
topic.top.words <span class="o">&lt;-</span> mallet.top.words<span class="p">(</span>topic.model<span class="p">,</span>
topic.words<span class="p">[</span>i<span class="p">,],</span> <span class="m">20</span><span class="p">)</span>
<span class="kp">print</span><span class="p">(</span>wordcloud<span class="p">(</span>topic.top.words<span class="o">$</span>words<span class="p">,</span>
topic.top.words<span class="o">$</span>weights<span class="p">,</span>
<span class="kt">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="m">.8</span><span class="p">),</span> rot.per<span class="o">=</span><span class="m">0</span><span class="p">,</span>random.order<span class="o">=</span><span class="bp">F</span><span class="p">,</span>
colors<span class="o">=</span>brewer.pal<span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="s">&quot;Dark2&quot;</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
<span class="p">}</span></code></pre></div>

<p>We can also try clustering plot based on shared words:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">hc <span class="o">&lt;-</span> hclust<span class="p">(</span>dist<span class="p">(</span>topic.words<span class="p">))</span>

<span class="p">(</span>dens <span class="o">&lt;-</span> as.dendrogram<span class="p">(</span>hc<span class="p">))</span>

plot<span class="p">(</span>dens<span class="p">,</span> edgePar<span class="o">=</span><span class="kt">list</span><span class="p">(</span>col <span class="o">=</span> <span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span> lty <span class="o">=</span> <span class="m">2</span><span class="o">:</span><span class="m">3</span><span class="p">),</span> dLeaf<span class="o">=</span><span class="m">1</span><span class="p">,</span> edge.root <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>

plot<span class="p">(</span>hclust<span class="p">(</span>dist<span class="p">(</span>topic.words<span class="p">)),</span> labels<span class="o">=</span>topics.labels<span class="p">)</span></code></pre></div>

<p>It seems to messy, let’s create a data.frame and calculate a similarity matrix:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic_docs <span class="o">&lt;-</span> <span class="kt">data.frame</span><span class="p">(</span>topic.docs<span class="p">)</span>

<span class="kp">names</span><span class="p">(</span>topic_docs<span class="p">)</span> <span class="o">&lt;-</span> docs<span class="o">$</span>id

topic_dist <span class="o">&lt;-</span> <span class="kp">as.matrix</span><span class="p">(</span>daisy<span class="p">(</span><span class="kp">t</span><span class="p">(</span>topic_docs<span class="p">),</span> metric <span class="o">=</span> <span class="s">&quot;euclidean&quot;</span><span class="p">,</span> stand <span class="o">=</span> <span class="kc">TRUE</span><span class="p">))</span></code></pre></div>

<p>The following does the trick to keep only closely related documents and avoid a dense diagram, otherwise it can become difficult to interpret. Change row values to zero if less than row minimum + row standard deviation</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">topic_dist<span class="p">[</span> <span class="kp">sweep</span><span class="p">(</span>topic_dist<span class="p">,</span> <span class="m">1</span><span class="p">,</span> <span class="p">(</span><span class="kp">apply</span><span class="p">(</span>topic_dist<span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="kp">min</span><span class="p">)</span> <span class="o">+</span> <span class="kp">apply</span><span class="p">(</span>topic_dist<span class="p">,</span><span class="m">1</span><span class="p">,</span>sd<span class="p">)</span> <span class="p">))</span> <span class="o">&gt;</span> <span class="m">0</span> <span class="p">]</span> <span class="o">&lt;-</span> <span class="m">0</span></code></pre></div>

<h4 id="gran-finale-1">Gran Finale</h4>
<p>Finally, we can use means to identify groups of similar documents, and further get names for each cluster</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">km <span class="o">&lt;-</span> kmeans<span class="p">(</span>topic_dist<span class="p">,</span> n.topics<span class="p">)</span>

alldocs <span class="o">&lt;-</span> <span class="kt">vector</span><span class="p">(</span><span class="s">&quot;list&quot;</span><span class="p">,</span> length <span class="o">=</span> n.topics<span class="p">)</span>

<span class="kr">for</span><span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span>n.topics<span class="p">){</span>
alldocs<span class="p">[[</span>i<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kp">names</span><span class="p">(</span>km<span class="o">$</span>cluster<span class="p">[</span>km<span class="o">$</span>cluster <span class="o">==</span> i<span class="p">])</span> <span class="p">}</span></code></pre></div>



</div>

<hr>
<p></p>

<div class="meta">
	
	
	<span class="categories">
		Published in categories
		
		<a href="/blog/categories/#r" title="r">r</a>&nbsp;
		
	</span>
	
	
	
	<span class="tags">
		Tagged with 
		
		<a href="/blog/tags/#R" title="R">R</a>&nbsp;
		
		<a href="/blog/tags/#LDA" title="LDA">LDA</a>&nbsp;
		
	</span>
	
</div>


<div class="previous_next">
	
	  <a href="/blog/r/2014/12/03/study-of-a-plot.html" title="Previous post">&larr; previous</a>
	
	&nbsp;&nbsp;
	
	  <a href="/blog/r/2015/06/23/network-analysis-with-igraph.html" title="Next post">next &rarr;</a>
	
	<br>
	<p>
	<a class="greenbutton" href="/blog/archive/" title="an archive of all posts">See all posts &rarr;</a>
	</p>
</div>


<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'daniel-marcelino'; // required: replace example with your forum short name

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	</div>
</div>


<div id="footer">

<div id="footer">
The contents of this blog is syndicated to <a href="http://www.r-bloggers.com">R-Bloggers</a> and <a href="http://www.statsblogs.com">Statsblogs</a> via <a href="http://danielmarcelino.com/atom.xml"> Atom/RSS feeds.</a> <br/><br/>
	&copy; <a href="/">Daniel Marcelino</a>. All contents under 
	<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">(CC) BY-NC-SA license</a>, 
	unless otherwise noted.
	<br/><br/>
	Did you find this site useful? If yes, consider helping me with my  
	<a href="http://amzn.com/w/2CCCV54KPGIA2" target="_blank">wishlist</a>.
</div>


</div>




		<!-- Google Analytics -->
		<script>
  		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  		  })(window, document,'script','//www.google-analytics.com/analytics.js','ga');

	  	  ga('create', 'UA-24186657-1', 'danielmarcelino.com');
  		  ga('send', 'pageview');

		</script>
		<!-- Google Analytics end -->

	</body>
</html>