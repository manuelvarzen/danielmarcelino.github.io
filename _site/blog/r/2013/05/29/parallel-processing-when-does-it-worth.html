<!DOCTYPE html>
<html lang="en">
	<head>
        <title>Parallel Processing: When does it worth? | Daniel Marcelino</title>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
        <meta name="keywords" content="methods, data analysis, R programming" />
        <meta name="author" content="Daniel Marcelino" />
        <meta name="description" content="Website of Daniel Marcelino" />
        <meta name="viewport" content="width=640px, initial-scale=1.0, maximum-scale=1.0">	
        <!-- favicon -->
        <link rel="shortcut icon" href="/images/favicon.ico">
        <!-- Google Fonts -->
		<link href="http://fonts.googleapis.com/css?family=Open+Sans:300italic,300,400,600" rel="stylesheet" type="text/css">
		<link href='http://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css" type="text/css" media="screen, projection" />
		<link rel="alternate" type="application/rss+xml" title="EnfoRced Data Analysis" href="/feed.xml">
	<link rel="alternate" type="application/rss+xml" title="EnfoRced Data Analysis" href="/feed.r.xml">
		<link rel="alternate" type="application/atom+xml" title="EnfoRced Data Analysis" href="/atom.xml" />
		
	
</head>

	<body>


		<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div class="header-page">

<h1><a href="/"><span style='font-weight: bold;'>Daniel Marcelino</span></a></h1></span>
<div id="nav">
	<ul>
	   <li><a href="/blog" class="current"><strong>Blog</strong></a></li>
		<li><a href="/about" class=""><strong>About</strong></a></li>
	<li><a href="/research" class=""><strong>Research</strong></a></li>
		<li><a href="/teaching" class=""><strong>Teaching</strong></a></li>
		<li><a href="/software" class=""><strong>Software</strong></a></li>		
	<li><a href="/randomwalks" class=""><strong>Random Walks</strong></a></li>		
	</ul>
</div>

</div>


<div class="clear"></div>

<div id="block">
	<div class="prose"> 
	<div class="post">
	<h1> Parallel Processing: When does it worth? </h1>
	<!-- <span class="date">Posted on 29 May 2013</span> -->
	<h5>Posted on May 29, 2013</h5>
	
	<p>Parallel computing is incredibly useful, but not every thing worths distribute across as many cores as possible.</p>

<!--more-->

<p><img src="/images/blog/2013/parallel.jpg" alt="Experiment Result" /></p>

<p>Most computers nowadays have few cores that incredibly help us with our daily computing duties. However, when statistical softwares do use parallelization for analyzing data faster? R, my preferred analytical package, does not take too much advantage of multicore processing by default. </p>

<p>In fact, R has been inherently a “single-processor” package until nowadays. Stata, another decent statistical package, allows for multicore processing, but not yet implemented for the default version. Additionally, Stata versions for multicore are quite expensive, for instance, if I want to use all of my quad-core computer power, I have to pay out $1,095 for a 4-core single license.</p>

<p>Fortunately, several packages exploiting the resources of parallel processing began to gain attention in the R community lately. Specially, because of many computationally demanding statistical procedures, such as bootstrapping and Markov Chain Monte Carlo, I guess. Nonetheless, the myth of fast computation has also leading for (miss)understanding by ordinary R users like me. Yet before distributing patches among CPUs, it is important to have it clear when and why parallel processing is helpful and which functions performs better for a given job.</p>

<p>First, what does exactly parallel do? Despite its complex implementation, the idea is incredible simple: parallelization simply distribute the work among two or more cores. It is done by packages that provide backend for the “foreach” function to work. The foreach function allows R to distribute the processes, each of which having access to the same shared memory; so the computer not get confused. For instance, in the program bellow, several instances of lapply-like function are able to access the processing units and then deal out with all the work.</p>

<p>Since not every task runs better in parallel there is not too many ready to use parallel processing functions in R. Additionally, distributing processes among the cores may cause computation overhead. That means we may lose computer time and memory, firstly by distributing, secondly by gathering the patches shared out among the processing units. Therefore, depending on the task (time and memory demand), parallel computing can be rather inefficient. It may take more time for dispatching the processes and gathering them back than the computation itself. Hence, counterintuitively, one might want to minimize the number of dispatches rather than distribute them.</p>

<p>Here, I’m testing a nontrivial computation instance for measuring computer performance on four relevant functions: the base <strong>lapply</strong>, <strong>mclapply</strong> from the “multicore“ package, <strong>parLapply</strong> from “snow” package, and <strong>sfLapply</strong> from “snowfall“ package. The last three functions essentially provide parallelized equivalent for the lapply. I use these packages for parallel computing the average for each column of a data frame built on the fly, but repeating this procedure 100 times for each data frame trial; so each trial demands different amount of time and memory for computing: the matrix size increases as 1K, 10K, 100K, 1M, and 10M rows. The program I used for simulate the data and perform all the tests can be found <a href="https://gist.github.com/danielmarcelino/5668701">here</a>. I used Emacs on a MacBook pro with 4-core and 8-G memory.</p>

<p><img src="/images/blog/2013/parallelfinal.jpeg" alt="Final Results" /></p>

<p>My initial experiment also included the “mpi.parLapply“ function from the “Rmpi” package. However, because it is outdated for running on R version 3.0, I decided for not including it in this instance.</p>

<p>Overall, running repetition tasks in parallel incurs overhead. Therefore, only if the process takes a significant amount of time and memory (RAM), parallelization can improve the overall performance. </p>

<p>The plots above, provide evidence when an individual process takes less than a second (<code>2.650/100 = 26.5 milliseconds </code> comparable to <code>13.419/100 = 134.2 milliseconds</code>) to be executed, the overhead of continually patching processes will decay the overall performance. For instance, in the first chart, the lapply function took less than one-third of the time of <strong>sfLapply</strong> to perform the same job. However, the pattern changes dramatically when the computer needs to repeat the same task with large vectors (&gt;= 1 million rows). </p>

<p>In fact, I noticed that all functions began to consume even more time computing  averages of big matrixes than I expected. But in a setting with 10 millions rows, the <strong>lapply</strong> function was dramatically inefficient: it took <code>1281.8/100 = 12.82 seconds</code> for each process, while the <strong>mclapply</strong> only took <code>525.4/100 = 5.254 seconds</code>.</p>



</div>

<hr>
<p></p>

<div class="meta">
	
	
	<span class="categories">
		Published in categories
		
		<a href="/blog/categories/#r" title="r">r</a>&nbsp;
		
	</span>
	
	
	
	<span class="tags">
		Tagged with 
		
		<a href="/blog/tags/#R" title="R">R</a>&nbsp;
		
		<a href="/blog/tags/#parallel computing" title="parallel computing">parallel computing</a>&nbsp;
		
	</span>
	
</div>


<div class="previous_next">
	
	  <a href="/blog/r/2012/01/10/looping-through-factor-variables.html" title="Previous post">&larr; previous</a>
	
	&nbsp;&nbsp;
	
	  <a href="/blog/r/2013/06/23/got-bootstrap.html" title="Next post">next &rarr;</a>
	
	<br>
	<p>
	<a class="greenbutton" href="/blog/archive/" title="an archive of all posts">See all posts &rarr;</a>
	</p>
</div>


<div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'daniel-marcelino'; // required: replace example with your forum short name

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	</div>
</div>


<div id="footer">

<div id="footer">
The contents of this blog is syndicated to <a href="http://www.r-bloggers.com">R-Bloggers</a> and <a href="http://www.statsblogs.com">Statsblogs</a> via <a href="http://danielmarcelino.com/atom.xml"> Atom/RSS feeds.</a> <br/><br/>
	&copy; <a href="/">Daniel Marcelino</a>. All contents under 
	<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">(CC) BY-NC-SA license</a>, 
	unless otherwise noted.
	<br/><br/>
	Did you find this site useful? If yes, consider helping me with my  
	<a href="http://amzn.com/w/2CCCV54KPGIA2" target="_blank">wishlist</a>.
</div>


</div>




		<!-- Google Analytics -->
		<script>
  		  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  		  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  		  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  		  })(window, document,'script','//www.google-analytics.com/analytics.js','ga');

	  	  ga('create', 'UA-24186657-1', 'danielmarcelino.com');
  		  ga('send', 'pageview');

		</script>
		<!-- Google Analytics end -->

	</body>
</html>