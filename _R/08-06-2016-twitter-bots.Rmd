---
title: "Playing with the streamR package and visualizing Twitter conversations"
author: "Daniel Marcelino"
date: "September 22, 2016"
output: html_document
---

This morning I started fooling around with the streamR package to access the Twitter Streaming API and see what data was captured and what could be done with it. For 90 minutes this morning I captured all tweets that originated in the United States - because I was searching for tweets within a specific geographic area, I necessarily missed tweets from those users not using geolocation.

This assumes that you have already gained authorization and credentials to use the Twitter API.

library(hadleyverse)
library(maps)
library(streamR)

# open stream and capture tweets from the United States
filterStream("tweetsUS90.json", locations = c(-125, 25, -66, 50), timeout = 5400, 
              oauth = my_oauth)
tweets.df <- parseTweets("tweetsUS90.json", verbose = FALSE)


After 90 minutes this morning I had gathered 172,738 tweets from 85,837 unique users. As an initial check on the data, we can map the locations of the users in the dataset. As expected, the majority of the users are found in metropolitan areas.

points <- data.frame(x = as.numeric(tweets.df$place_lon), y = as.numeric(tweets.df$place_lat))
points <- points[points$y > 25, ]

## plot out the usage on a map
xlim <- c(-124.738281, -66.601563)
ylim <- c(24.039321, 50.856229)
map("world", col="#E8E8E8", fill=TRUE, bg="white", lwd=0.4, xlim=xlim, ylim=ylim, interior=TRUE)
points(points, pch=16, cex=.10, col="red")
map("state", fill=FALSE, bg="white", add = TRUE)


Based on the plot, we can see that the location filter properly captured data within the specified longitude and latitude. But this is not a density plot, so the northeastern corridor is a bit misleading. In fact, users in the South more than doubled the number of tweets of every other region over the timeframe in question. You can see in the (ugly) table below the top 20 states and the number of tweets captured over the 90 minute span.


The filterStream function also captures the type of device or platform from which the tweet was generated. This allows us to investigate the most popular platforms and devices for each region. The plot below captures the top 5 most popular platform types for each region, and in each the iPhone is certainly the most popular. The top 5 platforms are consistent across region, and while three of the top 5 in each region are native to Twitter, two of are note. First, Instagram, once blocked from direct sharing on Twitter, is now a source of heavy user interaction. Second, more popular than Instagram is TweetMyJOBS, an online recruiting company posting jobs through social media. As an anecdotal point of evidence, it appears the economy is doing well.


The streamR package also captures the activity and popularity of each of these users. A histogram of users’ follower count demonstrates a significant right skew. The mean number of followers for the sample is 1835, but the median number is only 436 and users in the 90th percentile of followers have only 1734. The mean here is being inflated by the super users of Twitter; within this sample, we find accounts for perezhilton.com, NASA, Slate, and Domino’s pizza. Each of these accounts are far into the tail of the distribution, with the perezhilton account clocking in with over 6 million followers.

Visualizing the number of followers for each account as a function of account activity indicates that there is not a strong relationship between frequent updates and earning followers. The correlation between the two is just .07. An example of this comes from the most frequent poster over the duration of the data collected. In 90 minutes, a spam-bot account posted 125 times. This account was created almost three years ago, and in that time has posted over 37,000 status updates. It has 6 followers. 

Finally, we can use this data to visualize the conversations that are being had on Twitter. Are people shouting into the void on the platform? Are they using it to interact with people near or far? The streamR package allows us to begin thinking about this because it captures whether or not a tweet was in reply to another user (this does not necessarily imply a two-way conversation, but it was at least in reference to a previous tweet; it wasn’t just a random thought thrown at random into the ether). With this data, one can then cross reference the recipient to see if they had previously sent a geocoded tweet during the span of data collected. With only a 90 minute window, the number of positive cases was low - 1254 “conversations” were had - but it does provide some data to play with.




```{r}
## Playing around with streamR and Twitter API ## 
library(streamR)
library(hadleyverse)
library(maps)

# --------------------------- # 
# Setting up the handshake 
library(ROAuth)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- api_key
consumerSecret <- api_secret
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret,  
                             requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
## at this point enter the Twitter pin  into the R console
# --------------------------- #

#load("my_oauth.Rdata")
#setwd("")

# --------------------------- #
# grab data for 90 minutes in United States
# geoJSON coordinates for other cities can be found 
# at https://gist.github.com/taylorgrant/ba1d49272e4bf1da5a0a
filterStream("tweetsUS90.json", locations = c(-125, 25, -66, 50), timeout = 5400, 
              oauth = my_oauth)
tweets_df <- parseTweets("tweetsUS90.json", verbose = FALSE)

## plotting out the usage on a map
points <- data.frame(x = as.numeric(tweets_df$place_lon), y = as.numeric(tweets_df$place_lat))
points <- points[points$y > 25, ]
xlim <- c(-124.738281, -66.601563)
ylim <- c(24.039321, 50.856229)

map("world", col="#E8E8E8", fill=TRUE, bg="white", lwd=0.4, xlim=xlim, ylim=ylim, interior=TRUE)
points(points, pch=16, cex=.10, col="red")
map("state", fill=FALSE, bg="white", add = TRUE)

# ------------------ #  
# What are people using to tweet?
# will need these eventually
state1 <- data.frame(state.name, state.abb, stringsAsFactors = FALSE)
state2 <- data.frame(state.abb, state.region, stringsAsFactors = FALSE)

# clean source of tweet
type <- data.frame(tweets_df$source, stringsAsFactors = FALSE)
type <- type %>% mutate(tweets_df.source = str_replace_all(tweets_df.source, '.*">', ""),
                        tweets_df.source = str_replace_all(tweets_df.source, '</a>', ""),
                        tweets_df.source = str_replace_all(tweets_df.source, 'SafeTweet by ', ""))

# find regions
local <- tweets_df$full_name
local <- str_split(local, ", ")
local <- do.call(rbind, local)
local <- local[,1:2]
local <- data.frame(local, stringsAsFactors = FALSE)

# merge in state.abb, convert USA to state, drop any that aren't states
regions <-  left_join(local, state1, by = c("X1" = "state.name")) %>%
  mutate(X2 = ifelse(X2 == "USA", state.abb, X2)) %>% 
  select(-state.abb) %>% data.frame(type) %>% 
  filter(!str_length(X2) > 2) %>% 
  left_join(state2, by = c("X2" = "state.abb"))
colnames(regions) <- c("area", "st_abb", "source", "region")

# DC is considered NA, change to Northeast
regions$region[is.na(regions$region)] <- "Northeast"

# summarize and put into dotplot
df_regions <- regions %>%
  group_by(region, source) %>% 
  summarise(total = n()) %>% data.frame() %>%
  arrange(desc(total))

top5 <- df_regions %>%
  group_by(region) %>%
  top_n(n = 5) %>%
  arrange(region)

ggplot(top5, aes(x = total, y = region, colour = source)) +
  geom_point(size=5) + 
  labs(title = "How do people Tweet", x = "Total", y = "Region") + theme_bw()

# ---------------- #
## follower count and usage
summary(tweets_df$followers_count)
quantile(tweets_df$followers_count, p = seq(0,.95, .05))

#density on y-axis with density plot
g3<-ggplot(tweets_df, aes(followers_count)) + 
  geom_histogram(aes(y = ..density..), fill=NA, color="black", binwidth=50) + 
  xlim(0, 10000) + geom_density(color="blue") +
  theme_bw()   #nicer looking 
g3    

popular <- tweets_df %>%
  distinct(screen_name) %>%
  filter(followers_count > quantile(followers_count, p=.9)) %>% 
  mutate(twit_ratio = followers_count / statuses_count) %>%
  arrange(desc(followers_count)) %>% 
  data.frame()

efficiency <- tweets_df %>%
  distinct(screen_name) %>%
  mutate(twit_ratio = followers_count / statuses_count) %>%
  arrange(desc(twit_ratio)) %>% 
  data.frame()

# dplyr mutate doesn't work with date/time format? 
efficiency$date <- strptime(efficiency$user_created_at, format =  "%a %b %d %H:%M:%S %z %Y")
efficiency$acct_age <- (as.numeric(now() - efficiency$date))/24/7 # age of account in weeks
efficiency$percentile <- ecdf(efficiency$followers_count)(efficiency$followers_count) # add in efficiency percentiles

g6 <- ggplot(data = efficiency, aes(x = statuses_count, y = followers_count)) + 
  geom_point(size = 1.2, alph = .6, col="#4e4e4e") + ylim(0,4000) + 
  xlim(0,40000) + stat_smooth(size = 2) + theme_bw() #90th percentile for status updates
g6


# ------------ # 
# find those who were tweeted at in reply to previous tweeet 
# and who allow geolocation
to <- tweets_df$in_reply_to_screen_name
sent_to <- to %in% tweets_df$screen_name
receive <- to[sent_to] # 5655 tweets to someone in the data

s_df <- tweets_df[tweets_df$in_reply_to_screen_name %in% receive,] # geolocation of those who sent tweets
r_df <- tweets_df[tweets_df$screen_name %in% receive,] # geolocation of those who received tweets

geo <- data.frame(s_df$place_lat, s_df$place_lon, s_df$screen_name, 
                  s_df$in_reply_to_screen_name, s_df$id_str, stringsAsFactors = FALSE)

geo_receive <- data.frame(r_df$place_lat, r_df$place_lon, 
                          r_df$screen_name, stringsAsFactors = FALSE)

inter_df <- left_join(geo, geo_receive, by = c("s_df.in_reply_to_screen_name" = "r_df.screen_name"))
`%notin%` <- function(x,y) !(x %in% y)
inter_df <- inter_df[inter_df$s_df.screen_name %notin% inter_df$s_df.in_reply_to_screen_name,]

inter_df <- inter_df %>% distinct(s_df.id_str)

colnames(inter_df) <- c("from_lat", "from_lon", "at", "from", 'id', "to_lat", "to_lon")

## plot the data 
library(maps)
library(geosphere)

# Calculate distance in kilometers between two points (this is rough, but works for our purposes)
earth.dist <- function (long1, lat1, long2, lat2) {
  rad <- pi/180
  a1 <- lat1 * rad
  a2 <- long1 * rad
  b1 <- lat2 * rad
  b2 <- long2 * rad
  dlon <- b2 - a2
  dlat <- b1 - a1
  a <- (sin(dlat/2))^2 + cos(a1) * cos(b1) * (sin(dlon/2))^2
  c <- 2 * atan2(sqrt(a), sqrt(1 - a))
  R <- 6378.145
  d <- R * c
  return(d)
}

pal <- colorRampPalette(c("#d9d9d9", "white"))
colors <- pal(100)
xlim <- c(-124.738281, -66.601563)
ylim <- c(24.039321, 50.856229)
map("world", col="#232323", fill=TRUE, bg="black", lwd=0.01, xlim=xlim, ylim=ylim, interior=FALSE)
for (i in 1:3) { 
  for (j in 1:dim(inter_df)[1]) {
    inter <- gcIntermediate(c(inter_df[j,]$to_lon, inter_df[j,]$to_lat), c(inter_df[j,]$from_lon, inter_df[j,]$from_lat), 
                            n=100, addStartEnd = TRUE)
    colindex <- round(earth.dist(inter_df[j,2], inter_df[j,1], inter_df[j,7], inter_df[j,6]) / 4435 * length(colors) +1 )
    lines(inter, col=colors[colindex], lwd=0.05)
  }
}
```



```{r}
# -------------------------------------# 
# A twitter bot that periodically tweets 
# out the surf conditions in El Porto, CA. 
# After sunset, the bot tweets out the 6AM
# surf forecast for the next day. 
#     @ElPortoSurf
# -------------------------------------#

#devtools::install_github("geoffjentry/twitteR")
library(twitteR)
library(httr)
library(base64enc)
library(rvest)
library(lubridate)
library(stringr)
library(dplyr)

# using this to store the API locally
# http://www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/
# # ---------------------- #
# credentials <- c(
#   "twitter_api_key=blahblahblah_blah",
#   "twitter_api_secret=blahblahblah_blahblahblahblah_blah",
#   "twitter_access_token=blahblahblah_blahblahblahblah_blah",
#   "twitter_access_token_secret=blahblahblah_blah"
# )
# 
# fname <- paste0(normalizePath("~/"),"/.Renviron")
# writeLines(credentials, fname)
# 
# browseURL(fname)
# ---------------------- #

# setup authentication
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")

options(httr_oauth_cache = TRUE)

# httr:::guess_cache()
# httr:::use_cache()
# getwd()

setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)

# --------------------------- # 
# Scrape Data: Surf Report for El Porto (surfline.com) # 
url <- "http://www.surfline.com/surf-report/el-porto-southern-california_4900/"

# --------------------------- #
## grab sunrise and sunset
rise_set <- read_html(url) %>% 
  html_nodes("div:nth-child(16) span") %>% 
  html_text()

sunrise <- str_extract(rise_set, "([1-9][:][0-5][0-9][A][M])")
sunrise <- strptime(sunrise, "%I:%M %p")
sunset <- str_extract(rise_set, "([1-9][:][0-5][0-9][P][M])")
sunset <- strptime(sunset, "%I:%M %p")

cur_time <- now()
day_plus1 <- cur_time + days(1)
weekday <- as.character(wday(day_plus1, label=TRUE, abbr=FALSE))
month <- month(day_plus1)
day <- day(day_plus1)

# --------------------------- #
# Morning - Current conditions
if (cur_time > sunrise & cur_time < sunrise + hours(2)) {
  ## Current Conditions
  cast <- read_html(url) %>%
    html_nodes("#observed-spot-conditions , #observed-wave-description, #observed-wave-range") %>%
    html_text() 
  
  # clean up conditions
  wave <- cast[[1]] %>% str_replace("m", "") %>% str_c("ft")
  height <- cast[[2]] %>% str_replace_all(("\n"), "") %>% str_replace("-", "") %>%
    str_trim("both") 
  conditions <- cast[[3]] %>% str_replace("Conditions", "") %>% str_trim("right")
  
  ## Time of conditions 
  full <- read_html(url) %>%
    html_nodes("strong") %>%
    html_text()
  
  # clean up time 
  date <- gsub("\n","", full[11])
  time <- str_extract(date, "((1)?[0-9][:][0-9][0-9][a-p][m])")
  
  # change display time so that the reporting time is 
  # not always the same 
  time2 <- strptime(time, '%R')
  dis_time <- ifelse((time2 - Sys.time() > -1), strftime(Sys.time(),"%I:%M %p"), paste("As of", time, ""))
  
  ## Water temp
  h20temp <- read_html(url) %>%
    html_nodes(":nth-child(7) div:nth-child(2) span:nth-child(5)") %>% 
    html_text() %>%
    str_replace_all("\n", "") %>% 
    str_trim("both")
  
  morning_surf <- str_c(dis_time,": ", "Surf conditions are ", conditions,". ", 
                        "Waves are ", height,": ", wave,". ", "Water Temp: ",h20temp, ". #elporto #surf", "")
  morning_surf
  tweet(morning_surf)
}

# --------------------------- #
## Early Morning / Afternoon - Current Conditions 
if (cur_time > sunrise + hours(2) & cur_time < sunset) {
  current_url <- "http://magicseaweed.com/El-Porto-Beach-Surf-Report/2677/"
  current_url <- "http://magicseaweed.com/South-Beach-Surf-Report/298/"
  current <- read_html(current_url) %>%
    html_nodes(".msw-fc-current-v0 .row") %>%
    html_text()  %>% str_split("        ")
  
  current <- unlist(current)
  
  # break into lists for easy use
  # first drop "Wind Swell"
  cond <- current
  cond <- cond[!str_detect(cond, "Wind Swell")]
  cond <- cond[!str_detect(cond, "Secondary Swell")]
  cond[cond==""] <- NA
  cond <- cond[complete.cases(cond)]
  cond <- str_trim(cond, "both") %>%
    str_split(" ")
  
  waves <- cond[[1]]
  wind <- cond[[2]]
  swell <- cond[[3]]
  weather <- cond[[4]]
  
  current_surf <- str_c(strftime(Sys.time(),"%I:%M %p"),":"," Waves ", waves, " with a ", swell[2], " of ", 
                        swell[6], " at ", swell[8], ". ", wind[3], " wind at ", wind[1], ". ", weather[1], " and ",
                        weather[3], " Water temp: ", weather[5], weather[6], ". #elporto #surf")
  current_surf
  tweet(current_surf)
}
# --------------------------- #
# Predicted Conditions - for the scheduled tweet
if (cur_time > sunset & cur_time < sunrise + days(1)) {
  root <- "http://magicseaweed.com/El-Porto-Beach-Surf-Report/2677/#" 
  pred_url <- str_c(root, weekday, day, month,"")
  
  pred <- read_html(pred_url) %>%
    html_nodes("table") %>%
    .[[3]] %>%
    html_table(fill=TRUE, header = TRUE )
  
  ## want the prediction of the day in question for 
  ## the 6am time
  pred_tbl <- pred[22, c(1,2,5,6,7,14,16:18)]
  colnames(pred_tbl) <- c("Time", "Surf", "Swell", "Period", "Direction", "Wind", "Weather", 'Temp', 'Prob')
  onshore <- ifelse(pred_tbl$Direction > 210 & pred_tbl$Direction < 345, "Onshore", NULL)
  
  ## generate tweet text
  pred_surf <- str_c(pred_tbl$Time, " forecast for ", month,"/", day , ": ", "Surf ", pred_tbl$Surf, "; ", onshore, " Swell ", 
                     pred_tbl$Swell, " w/ ", pred_tbl$Period, " period; ", pred_tbl$Weather, " & ", pred_tbl$Temp, 
                     " #elporto #surf" )
  
  pred_surf
  tweet(pred_surf)
}

# # 
# # # create log entry
# line_c <- paste( as.character(Sys.time()), current_surf ,sep="\t" )
# line_p <- paste( as.character(Sys.time()), pred_surf ,sep="\t" )
# write(line_c, line_p, file="tweets.log", ncol=2, append=TRUE)

```


```{r}
# surfbot 2 - reply to DMs directed at @ElPortoSurf
library(twitteR)
library(dplyr)
library(readr)
library(stringr)
library(rvest)

# load api and credentials
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
options(httr_oauth_cache = TRUE)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

# genesis for data ID
#dms <- dmGet()
#x <- dms[[1]]
#id1 <- data.frame(id1 = as.numeric(x$getId()))
#write_tsv(id1, "id1")

# read in current id of last dm responded to
curr_id <- as.numeric(read_table("/Users/taylorgrant38/Documents/R Resources/Twitter/SurfBot/id1"))
words <- c('current', 'conditions')

# gather dms and pull out unique tweet ids
dms <- dmGet(sinceID = curr_id)
test_id <- sapply(dms, function(x) as.vector(as.numeric(x$getId())))

# test to see if there are any new dms
new <- length(test_id[test_id > curr_id])

if (new == 0) {
  NULL
}
if (new > 0) {
  # pull out screen_name of sender and text
  test_name <- sapply(dms, function(x) as.vector(x$getSenderSN()))[c(seq(new))]
  test_text <- sapply(dms, function(x) as.vector(x$getText()))[c(seq(new))]
  
  # test to see if DM matches our search phrase, keep those names that do
  final_name <- as.vector(test_name[which(str_detect(str_to_lower(test_text), 
                                                     str_to_lower(paste(words, collapse = ' '))) == TRUE)])
  
  # scraping current conditions
  if (length(final_name) > 0) {
    current_url <- "http://magicseaweed.com/El-Porto-Beach-Surf-Report/2677/"
    
    current <- read_html(current_url) %>%
      html_nodes(".msw-fc-current-v0 .row") %>%
      html_text()  %>% str_split("        ")
    current <- unlist(current)
    
    # break into lists for easy use (drop Wind Swell and Secondary Swell)
    cond <- current
    cond <- cond[!str_detect(cond, "Wind Swell")]
    cond <- cond[!str_detect(cond, "Secondary Swell")]
    cond[cond==""] <- NA
    cond <- cond[complete.cases(cond)]
    cond <- str_trim(cond, "both") %>%
      str_split(" ")
    
    waves <- cond[[1]]
    wind <- cond[[2]]
    swell <- cond[[3]]
    weather <- cond[[4]]
    
    current_surf <- str_c(strftime(Sys.time(),"%I:%M %p"),":"," Waves ", waves, " with a ", swell[2], " of ", 
                          swell[6], " at ", swell[8], ". ", wind[3], " wind at ", wind[1], ". ", weather[1], " and ",
                          weather[3], " Water temp: ", weather[5], weather[6], ". #elporto #surf")
    current_surf
    
    # make sure not sending multiple DMs to same account
    final_name <- unique(final_name)
   
     # send DM response to each account
    for (i in 1:length(final_name)) {
      dmSend(current_surf, final_name[i])
    }
  }
  else if (length(final_name) == 0) {
    NULL
  }
}
tmp_id <- data.frame(test_id[1])
write_tsv(tmp_id, "/Users/taylorgrant38/Documents/R Resources/Twitter/SurfBot/id1")
```

